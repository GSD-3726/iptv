import random
import requests
from lxml import etree
import os
import threading
import time
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# å¦‚éœ€ä½¿ç”¨ä»£ç†è¯·å–æ¶ˆæ³¨é‡Šï¼Œç¡®ä¿proxyTest.pyå­˜åœ¨
# from proxyTest import get_valid_proxies

# ========== æ–°å¢ï¼šä»£ç Bçš„æ ¸å¿ƒæµ‹é€Ÿå‡½æ•°ï¼ˆåŠ _aåç¼€é¿å…å‘½åå†²çªï¼‰ ==========
import asyncio
import re
import subprocess
from time import time
from urllib.parse import urljoin
import m3u8
from aiohttp import ClientSession, TCPConnector

def _parse_time_to_seconds_a(t: str) -> float:
    """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºç§’ï¼ˆé¿å…å‘½åå†²çªï¼‰"""
    if not t:
        return 0.0
    parts = [p.strip() for p in t.split(':') if p.strip() != ""]
    if not parts:
        return 0.0
    try:
        total = 0.0
        for i, part in enumerate(reversed(parts)):
            total += float(part) * (60 ** i)
        return total
    except Exception:
        return 0.0

def _try_extract_speed_from_ffmpeg_output_a(output: str) -> float | None:
    """ä»ffmpegè¾“å‡ºæå–é€Ÿåº¦ï¼ˆé¿å…å‘½åå†²çªï¼‰"""
    def parse_size_value_a(value_str: str, unit: str | None) -> float:
        try:
            val = float(value_str)
        except Exception:
            return 0.0
        if not unit:
            return val
        unit_lower = unit.lower()
        if unit_lower in ("b", "bytes"):
            return val
        if unit_lower in ("kib", "k"):
            return val * 1024.0
        if unit_lower in ("kb",):
            return val * 1000.0
        if unit_lower in ("mib", "mb"):
            return val * 1024.0 * 1024.0
        return val

    try:
        total_bytes = 0.0
        m_video = re.search(r"video:\s*([0-9]+(?:\.[0-9]+)?)\s*(KiB|MiB|kB|B|kb|KB)?", output, re.IGNORECASE)
        m_audio = re.search(r"audio:\s*([0-9]+(?:\.[0-9]+)?)\s*(KiB|MiB|kB|B|kb|KB)?", output, re.IGNORECASE)
        if m_video:
            total_bytes += parse_size_value_a(m_video.group(1), m_video.group(2))
        if m_audio:
            total_bytes += parse_size_value_a(m_audio.group(1), m_audio.group(2))

        m_time = re.search(r"time=\s*([0-9:\.]+)", output)
        if total_bytes > 0 and m_time:
            secs = _parse_time_to_seconds_a(m_time.group(1))
            if secs > 0:
                return total_bytes / secs / 1024.0 / 1024.0
    except Exception:
        pass

    try:
        m_lsize = re.search(r"Lsize=\s*([0-9]+(?:\.[0-9]+)?)\s*(KiB|kB|MiB|B|kb|KB)?", output, re.IGNORECASE)
        m_size = re.search(r"size=\s*([0-9]+(?:\.[0-9]+)?)\s*(KiB|kB|MiB|B|kb|KB)?", output, re.IGNORECASE)
        m_time = re.search(r"time=\s*([0-9:\.]+)", output)
        size_bytes = 0.0
        if m_lsize and m_lsize.group(1).upper() != "N/A":
            size_bytes = parse_size_value_a(m_lsize.group(1), m_lsize.group(2))
        elif m_size:
            size_bytes = parse_size_value_a(m_size.group(1), m_size.group(2))
        if size_bytes > 0 and m_time:
            secs = _parse_time_to_seconds_a(m_time.group(1))
            if secs > 0:
                return size_bytes / secs / 1024.0 / 1024.0
    except Exception:
        pass

    try:
        m_bitrate = re.search(r"bitrate=\s*([0-9\.]+)\s*k?bits/s", output)
        if m_bitrate:
            kbps = float(m_bitrate.group(1))
            return kbps / 8.0 / 1024.0
    except Exception:
        pass

    return None

async def ffmpeg_url_a(url, headers=None, timeout=10):
    """æ‰§è¡Œffmpegè·å–è¾“å‡ºï¼ˆé¿å…å‘½åå†²çªï¼‰"""
    headers_str = "".join(f"{k}: {v}\r\n" for k, v in (headers or {}).items())

    args = ["ffmpeg", "-t", str(timeout)]
    if headers_str:
        args += ["-headers", headers_str]
    args += ["-http_persistent", "0", "-stats", "-i", url, "-f", "null", "-"]

    proc = None
    try:
        proc = await asyncio.create_subprocess_exec(
            *args, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )
        out, err = await asyncio.wait_for(proc.communicate(), timeout=timeout)
        if err:
            return err.decode(errors="ignore")
        if out:
            return out.decode(errors="ignore")
        return None
    except asyncio.TimeoutError:
        if proc:
            proc.kill()
        return None
    except Exception:
        if proc:
            proc.kill()
        return None
    finally:
        if proc:
            await proc.wait()

async def get_speed_with_download_a(url: str, headers: dict = None, timeout: int = 10) -> dict[str, float | None]:
    """å¼‚æ­¥ä¸‹è½½æµ‹é€Ÿï¼ˆé¿å…å‘½åå†²çªï¼‰"""
    start_time = time()
    delay = -1
    total_size = 0
    session = ClientSession(connector=TCPConnector(ssl=False), trust_env=True)
    try:
        async with session.get(url, headers=headers, timeout=timeout) as response:
            if response.status != 200:
                raise Exception("Invalid response")
            delay = int(round((time() - start_time) * 1000))
            async for chunk in response.content.iter_any():
                if chunk:
                    total_size += len(chunk)
    except:
        pass
    finally:
        total_time = time() - start_time
        await session.close()
        return {
            'speed': total_size / total_time / 1024 / 1024 if total_time > 0 else 0,
            'delay': delay,
            'size': total_size,
            'time': total_time,
        }

async def get_m3u8_speed_a(url: str, headers: dict = None, timeout: int = 10) -> float:
    """è·å–m3u8é“¾æ¥çš„æµ‹é€Ÿç»“æœï¼ˆé€‚é…ä»£ç Aï¼‰"""
    try:
        # ä¸‹è½½å¹¶è§£æM3U8æ–‡ä»¶
        async with ClientSession(connector=TCPConnector(ssl=False), trust_env=True) as session:
            async with session.get(url, headers=headers, timeout=timeout) as response:
                if response.status != 200:
                    return 0.0
                m3u8_content = await response.text()
        
        m3u8_obj = m3u8.loads(m3u8_content)
        playlists = m3u8_obj.playlists
        segments = m3u8_obj.segments
        segment_urls = []

        # å¤„ç†å¤šçº§M3U8ï¼ˆé€‰æ‹©å¸¦å®½æœ€é«˜çš„å­playlistï¼‰
        if playlists:
            best_playlist = max(playlists, key=lambda p: p.stream_info.bandwidth)
            playlist_url = urljoin(url, best_playlist.uri)
            async with ClientSession(connector=TCPConnector(ssl=False), trust_env=True) as session:
                async with session.get(playlist_url, headers=headers, timeout=timeout) as response:
                    if response.status == 200:
                        playlist_content = await response.text()
                        media_playlist = m3u8.loads(playlist_content)
                        segment_urls = [urljoin(playlist_url, seg.uri) for seg in media_playlist.segments]
        else:
            segment_urls = [urljoin(url, seg.uri) for seg in segments]

        # æµ‹é€Ÿé€»è¾‘ï¼šä¼˜å…ˆæµ‹TSç‰‡æ®µï¼Œæ— ç‰‡æ®µåˆ™æµ‹M3U8æœ¬èº«
        if not segment_urls:
            res = await get_speed_with_download_a(url, headers, timeout)
            speed = res['speed']
        else:
            # æµ‹è¯•å‰5ä¸ªTSç‰‡æ®µï¼ˆå¼‚æ­¥å¹¶å‘ï¼‰
            tasks = [get_speed_with_download_a(ts_url, headers, timeout) for ts_url in segment_urls[:5]]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            total_size = sum(r['size'] for r in results if isinstance(r, dict))
            total_time = sum(r['time'] for r in results if isinstance(r, dict))
            
            speed = total_size / total_time / 1024 / 1024 if total_time > 0 else 0.0

            # ç‰‡æ®µæµ‹é€Ÿä¸º0æ—¶ï¼Œç”¨FFmpegè¾…åŠ©æµ‹é€Ÿ
            if round(speed, 2) == 0:
                ff_out = await ffmpeg_url_a(url, headers, timeout)
                if ff_out:
                    parsed_speed = _try_extract_speed_from_ffmpeg_output_a(ff_out)
                    if parsed_speed is not None and parsed_speed > 0:
                        speed = parsed_speed

        return speed
    except Exception as e:
        print(f"æµ‹é€Ÿå¤±è´¥ {url}: {e}")
        return 0.0
# ========== æ–°å¢ç»“æŸ ==========

def get_url(name):
    # proxy = get_valid_proxies()  # å¦‚éœ€ä»£ç†è¯·å–æ¶ˆæ³¨é‡Š
    user_agents = [
        'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:117.0) Gecko/20100101 Firefox/117.0',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Safari/537.36 Edg/116.0.1938.69',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6_3) AppleWebKit/537.36 (KHTML, like Gecko) Version/15.6 Safari/537.36',
        'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
        'Mozilla/5.0 (Linux; Android 12; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Mobile Safari/537.36',
        'Mozilla/5.0 (Android 12; Mobile; rv:117.0) Gecko/117.0 Firefox/117.0',
        'Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)',
        'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Safari/537.36',
        'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:117.0) Gecko/20100101 Firefox/117.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/116.0.5845.179 Chrome/116.0.5845.179 Safari/537.36',
        'Mozilla/5.0 (compatible; Konqueror/4.14; Linux) KHTML/4.14.2 (like Gecko)',
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Epiphany/42.3 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Safari/537.36 OPR/103.0.4928.47",
    ]
    user_agent = random.choice(user_agents)
    # é…ç½®ChromeOptionsä»¥å¯ç”¨æ— å¤´æ¨¡å¼
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument(f"user-agent={user_agent}")
    # chrome_options.add_argument(f"--proxy-server={proxy}")  # å¦‚éœ€ä»£ç†è¯·å–æ¶ˆæ³¨é‡Š

    # è®¾ç½®ChromeDriver
    driver = webdriver.Chrome(options=chrome_options)

    try:
        # æ‰“å¼€æŒ‡å®šé¡µé¢
        driver.get('http://tonkiang.us/')
        # ç­‰å¾…ç›´åˆ° ID ä¸º 'search' çš„å…ƒç´ å¯è¢«ç‚¹å‡»
        username_input = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, 'search'))
        )
        username_input.send_keys(f'{name}')
        submit_button = driver.find_element(By.NAME, 'Submit')
        submit_button.click()
    except Exception as e:
        print(f"æ‰¾ä¸åˆ°å…ƒç´ : {e}")

    try:
        # è·å–é¡µé¢çš„æºä»£ç 
        page_source = driver.page_source
        m3u8_list = []
        # å°† HTML è½¬æ¢ä¸º Element å¯¹è±¡
        root = etree.HTML(page_source)
        result_divs = root.xpath("//div[@class='resultplus']")
        print(f"è·å–æ•°æ®: {len(result_divs)}")
        # æå–m3u8é“¾æ¥
        for div in result_divs:
            for element in div.xpath(".//tba"):
                if element.text is not None:
                    m3u8_url = element.text.strip()
                    print(m3u8_url)
                    m3u8_list.append(m3u8_url)
                    with open('m3u8_list.txt', 'a', encoding='utf-8') as f:
                        f.write(f'{name},{m3u8_url}' + '\n')
    except requests.exceptions.RequestException as e:
        print(f"Error: è¯·æ±‚å¼‚å¸¸. Exception: {e}")
        pass

    # å…³é—­WebDriver
    driver.quit()
    return m3u8_list

# ========== æ ¸å¿ƒä¿®æ”¹ï¼šæ›¿æ¢download_m3u8çš„æµ‹é€Ÿé€»è¾‘ ==========
def download_m3u8(url, name, initial_url=None):
    try:
        # éªŒè¯M3U8é“¾æ¥æœ‰æ•ˆæ€§ï¼ˆä»…ä¸‹è½½å¤´éƒ¨ï¼Œä¸å®Œæ•´ä¸‹è½½ï¼‰
        response = requests.get(url, stream=True, timeout=15)
        response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        m3u8_content = response.text
    except requests.exceptions.Timeout as e:
        print(f"{url}\nError: è¯·æ±‚è¶…æ—¶. Exception: {e}")
        return
    except requests.exceptions.RequestException as e:
        print(f"{url}\nError: è¯·æ±‚å¼‚å¸¸. Exception: {e}")
        return
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return
    else:
        # ä½¿ç”¨ä»£ç Bçš„å¼‚æ­¥æµ‹é€Ÿé€»è¾‘
        try:
            # æ‰§è¡Œå¼‚æ­¥æµ‹é€Ÿå‡½æ•°ï¼ˆåŒæ­¥è°ƒç”¨å¼‚æ­¥ï¼‰
            average_speed = asyncio.run(get_m3u8_speed_a(url, headers=None, timeout=15))
            print(f"---{name}---Average Download Speed: {average_speed:.2f} MB/s")
        except Exception as e:
            print(f"æµ‹é€Ÿå¼‚å¸¸ {url}: {e}")
            average_speed = 0.0

        # é€Ÿåº¦é˜ˆå€¼åˆ¤æ–­ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
        if average_speed >= speed:
            valid_url = initial_url if initial_url is not None else url
            if not os.path.exists(f'{TV_name}'):
                os.makedirs(f'{TV_name}')
            with open(os.path.join(f'{TV_name}', f'{name}.txt'), 'a', encoding='utf-8') as file:
                file.write(f'{name},{valid_url}\n')
            print(f"---{name}---é“¾æ¥æœ‰æ•ˆæºå·²ä¿å­˜---\n"
                  f"----{valid_url}---")
            return

def detectLinks(name, m3u8_list):
    thread = []
    for m3u8_url in m3u8_list:
        t = threading.Thread(target=download_m3u8, args=(m3u8_url, name,))
        t.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
        t.start()
        thread.append(t)
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in thread:
        try:
            print(f"Waiting for thread {t} to finish")
            t.join(timeout=10)  # ç­‰å¾…çº¿ç¨‹è¶…æ—¶
        except Exception as e:
            print(f"Thread {t.name} raised an exception: {e}")

def mer_links(tv):
    # è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ txt æ–‡ä»¶
    txt_files = [f for f in os.listdir(os.path.join(current_directory, f'{tv}'))]
    print(txt_files)
    # æ‰“å¼€åˆå¹¶åçš„æ–‡ä»¶
    with open(output_file_path, 'a', encoding='utf-8') as output_file:
        output_file.write(f'{tv},#genre#' + '\n')
        for txt_file in txt_files:
            file_path = os.path.join(os.path.join(current_directory, f'{tv}'), txt_file)
            # è¯»å–å¹¶å†™å…¥å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as input_file:
                file_content = input_file.read()
                output_file.write(file_content)
                output_file.write('\n')

    print(f'Merged content from {len(txt_files)} files into {output_file_path}')

def re_dup_ordered(filepath):
    from collections import OrderedDict
    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    with open(filepath, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    # ä¿æŒåŸå§‹é¡ºåºçš„å»é‡
    unique_lines_ordered = list(OrderedDict.fromkeys(lines))
    # å†™å›æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as file:
        file.writelines(unique_lines_ordered)
    print('-----ç›´æ’­æºå»é‡å®Œæˆï¼------')

def re_dup(filepath):
    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    with open(filepath, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    # è¿‡æ»¤æ‰åŒ…å« 'null' çš„è¡Œ
    filtered_lines = [line for line in lines if 'null' not in line]

    # å­—å…¸å»é‡
    unique_lines = {}
    for line in filtered_lines:
        parts = line.strip().split(',')
        if len(parts) == 2:
            channel_name, url = parts[0].strip(), parts[1].strip()
            if url not in unique_lines:
                unique_lines[url] = line

    # å†™å›æ–‡ä»¶
    unique_lines_ordered = list(unique_lines.values())
    with open(filepath, 'w', encoding='utf-8') as file:
        file.writelines(unique_lines_ordered)
    print('-----ç›´æ’­æºå»é‡å®Œæˆï¼------')

if __name__ == '__main__':
    speed = 1  # é€Ÿåº¦é˜ˆå€¼ï¼ˆMB/sï¼‰
    # è·å–å½“å‰å·¥ä½œç›®å½•
    current_directory = os.getcwd()
    # æ„é€ ä¸Šçº§ç›®å½•çš„è·¯å¾„
    parent_dir = os.path.dirname(current_directory)
    output_file_path = os.path.join(parent_dir, 'live.txt')
    # æ¸…ç©ºæ–‡ä»¶
    with open(output_file_path, 'w', encoding='utf-8') as f:
        pass
    with open('m3u8_list.txt', 'w', encoding='utf-8') as file:
        pass
    tv_dict = {}
    # ç›®æ ‡é¢‘é“åˆ†ç±»
    TV_names = ['ğŸ‡¨ğŸ‡³å¤®è§†é¢‘é“']
    for TV_name in TV_names:
        # åˆ é™¤å†å²æµ‹è¯•è®°å½•
        if os.path.exists(TV_name):
            import shutil
            try:
                shutil.rmtree(TV_name)
                print(f"Folder '{TV_name}' deleted successfully.")
            except OSError as e:
                print(f"Error deleting folder '{TV_name}': {e}")
        time.sleep(1)
        # åˆ›å»ºç›®å½•
        if not os.path.exists(TV_name):
            os.makedirs(TV_name)
        # è¯»å–é¢‘é“åç§°
        with open(f'{TV_name}.txt', 'r', encoding='utf-8') as file:
            names = [line.strip() for line in file]
            for name in names:
                m3u8_list = get_url(name)
                tv_dict[name] = m3u8_list
                print(name)
            print('---------å­—å…¸åŠ è½½å®Œæˆï¼------------')
        # å¤šçº¿ç¨‹æµ‹é€Ÿ
        for name, m3u8_list in tv_dict.items():
            detectLinks(name, m3u8_list)
        # åˆå¹¶æœ‰æ•ˆç›´æ’­æº
        mer_links(TV_name)
        tv_dict.clear()

    time.sleep(10)
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆä»£ç Bé€»è¾‘ä¸ç”Ÿæˆvideo.tsï¼Œå¯æ³¨é‡Šï¼‰
    if os.path.exists('video.ts'):
        os.remove('video.ts')
    # ç›´æ’­æºå»é‡
    re_dup_ordered(output_file_path)

    sys.exit()
